{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Tokenizer Class\n",
    "\n",
    "In this notebook we take a closer look at the `Tokenizer` class. We won't cover all methods and attributes, but only those that are likely to be useful when used in association with the `from_pretrained` method.\n",
    "\n",
    "All pre-trained tokenizers are instances of the `PreTrainedTokenizer` base class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, PreTrainedTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(isinstance(tokenizer, PreTrainedTokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizers allow:\n",
    "\n",
    "1. Tokenizing, i.e., converting a string into the individual text tokens.\n",
    "2. Numericalizing, i.e., converting the individual text tokens into integer IDs.\n",
    "3. Performing the opposite conversion, i.e., from ID to text.\n",
    "4. Adding new tokens to the vocabulary of the tokenizer.\n",
    "5. Managing special tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "\n",
    "The Bert tokenizer ships with a vocabulary of ~30k tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a token is not present, it is split into sub-units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['mor', '##ph', '##og', '##rani', '##tic'],\n",
       " ['sp', '##lice', '##oso', '##mic'],\n",
       " ['z', '##ly', '##x', '##olo', '##tl']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.tokenize(s) for s in ['Morphogranitic', 'Spliceosomic', 'Zlyxolotl']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Tokens\n",
    "\n",
    "The `add_special_tokens` method is used when the special tokens are not already in the vocabulary. For example, a GPT-2 model does not have a `<CLS>` token. Its argument is a dictionary whose keys must be in `['bos_token', 'eos_token', 'unk_token', 'sep_token', 'pad_token', 'cls_token', 'mask_token', 'additional_special_tokens'].`\n",
    "\n",
    "In almost all practical cases, we can ignore this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize vs. encode vs. encode_plus vs. batch_encode_plus vs. prepare_for_model\n",
    "\n",
    "### tokenize\n",
    "\n",
    "`tokenizer.tokenize` converts a string into a sequence of string tokens. It splits the sentence into words or sub-word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'does', 'token', '##ize', 'do', '?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('What does tokenize do?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encode\n",
    "\n",
    "Encode converts a string in a sequence of integer IDs. It is similar, but not identical, to `self.convert_tokens_to_ids(self.tokenize(text))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2054, 2515, 4372, 16044, 2079, 1029, 102]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_string = 'What does encode do?'\n",
    "tokenizer.encode(my_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2054, 2515, 4372, 16044, 2079, 1029]\n",
      "[2054, 2515, 4372, 16044, 2079, 1029]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(my_string)))\n",
    "print(tokenizer.encode(my_string, add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `encode` by default adds the special tokens at the beginning and at the end of the sequence. It can do more than this. It can truncate the sequence, and to max length, and return tensors instead of lists (`'pt'` for PyTorch and `'tf'` for TensorFlow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2054, 2515, 102]\n",
      "[101, 2054, 2515, 4372, 16044, 2079, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(my_string, max_length=4))\n",
    "print(tokenizer.encode(my_string, max_length=16, pad_to_max_length=True))\n",
    "print(type(tokenizer.encode(my_string, return_tensors='pt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encode_plus\n",
    "\n",
    "`encode_plus` does even more. It returns a dictionary structured as follows:\n",
    "\n",
    "```\n",
    "{\n",
    "  input_ids: list[int],\n",
    "  token_type_ids: list[int] if return_token_type_ids is True (default)\n",
    "  attention_mask: list[int] if return_attention_mask is True (default)\n",
    "  overflowing_tokens: list[int] if a ``max_length`` is specified and return_overflowing_tokens is True\n",
    "  num_truncated_tokens: int if a ``max_length`` is specified and return_overflowing_tokens is True\n",
    "  special_tokens_mask: list[int] if ``add_special_tokens`` if set to ``True`` and return_special_tokens_mask is True\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2054, 2515, 4372, 16044, 2079, 1029, 102],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(my_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101,\n",
       "  2054,\n",
       "  2515,\n",
       "  4372,\n",
       "  16044,\n",
       "  2079,\n",
       "  1029,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(my_string, max_length=16, pad_to_max_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overflowing_tokens': [4372, 16044, 2079, 1029],\n",
       " 'num_truncated_tokens': 4,\n",
       " 'input_ids': [101, 2054, 2515, 102],\n",
       " 'token_type_ids': [0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(my_string, max_length=4, return_overflowing_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch_encode_plus\n",
    "\n",
    "`tokenizer.batch_encode_plus` can operate on a list of strings (a batch of inputs) and perform many, but not all of the operations described above. Importantly, it can return tensor, attention masks and perform truncation but it does **not** perform padding. An obvious question is: how can it add attention masks if there is no padding? Simple, it just return a list of 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[2028, 6251, 2000, 3627, 2068, 2035, 999],\n",
       "  [2026, 14829, 2064, 6170, 2066, 11113, 3676, 1012],\n",
       "  [2339, 2572, 1045, 2467, 7501, 1029]],\n",
       " 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0]],\n",
       " 'attention_mask': [[1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_batch = ['One sentence to rule them all!',\n",
    "            'My socks can sing like Abba.',\n",
    "            'Why am I always hungry?'\n",
    "           ]\n",
    "tokenizer.batch_encode_plus(my_batch, max_length=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shall we encode by hand?\n",
    "\n",
    "The main problem with `batch_encode_plus` is that it cannot pad the sequences. Without padding the attention mask is useless. We could process each sentence via `encode_plus`. There is probably no point in returning tensors at this stage, as we have to extract the input IDs and the attention masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input_ids': [101, 2028, 6251, 2000, 3627, 2068, 2035, 999, 102, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]}, {'input_ids': [101, 2026, 14829, 2064, 6170, 2066, 11113, 3676, 1012, 102, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]}, {'input_ids': [101, 2339, 2572, 1045, 2467, 7501, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]}]\n"
     ]
    }
   ],
   "source": [
    "encoded_batch = [tokenizer.encode_plus(s, max_length=16, \n",
    "                       pad_to_max_length=True) for s in my_batch]\n",
    "print(encoded_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101, 2028, 6251, 2000, 3627, 2068, 2035, 999, 102, 0, 0, 0, 0, 0, 0, 0], [101, 2026, 14829, 2064, 6170, 2066, 11113, 3676, 1012, 102, 0, 0, 0, 0, 0, 0], [101, 2339, 2572, 1045, 2467, 7501, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "input_ids = [x['input_ids'] for x in encoded_batch]\n",
    "attn_mask = [x['attention_mask'] for x in encoded_batch]\n",
    "print(input_ids)\n",
    "print(attn_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can easily generate batches of input IDs and attention masks that can then be converted into PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2028,  6251,  2000,  3627,  2068,  2035,   999,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2026, 14829,  2064,  6170,  2066, 11113,  3676,  1012,   102,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2339,  2572,  1045,  2467,  7501,  1029,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor(input_ids))\n",
    "print(torch.tensor(attn_mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From IDs to strings\n",
    "\n",
    "The reverse mapping produces a string starting from a list of integer IDs. Let's see how.\n",
    "\n",
    "The `convert_ids_to_tokens` method returns the individual (sub)word tokens associated with the indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'my',\n",
       " 'socks',\n",
       " 'can',\n",
       " 'sing',\n",
       " 'like',\n",
       " 'ab',\n",
       " '##ba',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(input_ids[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can skip the special tokens during this conversion, but we still have sub-word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my', 'socks', 'can', 'sing', 'like', 'ab', '##ba', '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(input_ids[1], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `convert_tokens_to_string` does what the name suggests, and puts together the sub-word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my socks can sing like abba .'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_string(\n",
    "    tokenizer.convert_ids_to_tokens(input_ids[1], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same result can be obtained with the `decode` method, which, by default, returns also the special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my socks can sing like abba.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids[1], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare_for_model\n",
    "\n",
    "`prepare_for_model` is a strange function. It takes a sequence of input ids (or a pair), adds special tokens, truncates sequences, takes care of the special tokens, it can do padding, truncation, can return an attention mask and can return tensors. However the input has to be already numericalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2054, 2515, 4372, 16044, 2079, 1029, 102]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2054,  2515,  4372, 16044,  2079,  1029,   102,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_string = tokenizer.encode(my_string, add_special_tokens=True)\n",
    "print(encoded_string)\n",
    "tokenizer.prepare_for_model(encoded_string, add_special_tokens=False, max_length=16, \n",
    "                            pad_to_max_length=True, return_attention_mask=True,\n",
    "                            return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it does not work with batches, so there is no clear advantage, at least at a first glance, compared with the previous approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
